import os
import time
import json
import threading
import requests
from bs4 import BeautifulSoup
from datetime import datetime
from flask import Flask
import openai
import gspread
from oauth2client.service_account import ServiceAccountCredentials

# ğŸ§  ì„¤ì •
BOT_TOKEN = '8059473480:AAHWayTZDViTfTk-VtCAmPxvYAmTrjhtMMs'
CHAT_ID = '2037756724'
SHEET_NAME = 'ë””ë§ˆë‹ˆì½” ë‰´ìŠ¤ íŠ¸ë˜ì»¤'
MAX_SEND_PER_LOOP = 3  # í…ŒìŠ¤íŠ¸ë¼ì„œ ì‘ê²Œ
LINK_CACHE_FILE = 'old_links.json'
openai.api_key = os.environ.get("OPENAI_API_KEY")

# âœ… Flask ì„œë²„
app = Flask(__name__)
@app.route('/')
def home():
    return "ğŸŸ¢ ë””ë§ˆë‹ˆì½” ë‰´ìŠ¤ë´‡ í…ŒìŠ¤íŠ¸ ì‘ë™ ì¤‘!"

# âœ… êµ¬ê¸€ ì‹œíŠ¸ ì—°ê²°
def connect_google_sheet(sheet_name):
    key_json = os.environ.get('GOOGLE_KEY_JSON')
    if not key_json:
        print("âŒ GOOGLE_KEY_JSON í™˜ê²½ë³€ìˆ˜ ì—†ìŒ!")
        exit()
    key_dict = json.loads(key_json)
    scope = ['https://spreadsheets.google.com/feeds', 'https://www.googleapis.com/auth/drive']
    creds = ServiceAccountCredentials.from_json_keyfile_dict(key_dict, scope)
    client = gspread.authorize(creds)
    return client.open(sheet_name)

sheet = connect_google_sheet(SHEET_NAME)

def get_daily_worksheet(sheet):
    today = datetime.now().strftime('%Y-%m-%d')
    try:
        worksheet = sheet.worksheet(today)
    except gspread.exceptions.WorksheetNotFound:
        worksheet = sheet.add_worksheet(title=today, rows="1000", cols="6")
        worksheet.append_row(["ê¸°ë¡ì‹œê°„", "ë‰´ìŠ¤ì œëª©", "ìš”ì•½", "ê°ì„±", "ë§í¬", "ì–¸ë¡ ì‚¬"])
    return worksheet

def log_to_sheet(sheet, title, summary, sentiment, link, press):
    worksheet = get_daily_worksheet(sheet)
    now = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    try:
        worksheet.append_row([now, title, summary, sentiment, link, press])
        print(f"[ì‹œíŠ¸ ê¸°ë¡ë¨] {title}")
    except Exception as e:
        print(f"âŒ ì‹œíŠ¸ ê¸°ë¡ ì‹¤íŒ¨: {e}")

def load_old_links():
    try:
        with open(LINK_CACHE_FILE, 'r') as f:
            return json.load(f)
    except:
        return []

def save_old_links(links):
    with open(LINK_CACHE_FILE, 'w') as f:
        json.dump(links[-100:], f)

def get_live_news():
    url = "https://news.naver.com/main/list.naver?mode=LSD&mid=sec&sid1=001"  # ì†ë³´ ì„¹ì…˜
    res = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'})
    soup = BeautifulSoup(res.text, 'html.parser')

    news_list = []
    for li in soup.select("ul.type06_headline li"):
        a_tag = li.select_one("a")
        press = li.select_one(".writing")
        if a_tag and press:
            title = a_tag.get_text(strip=True)
            link = a_tag['href']
            press_name = press.get_text(strip=True)
            news_list.append((title, link, press_name))
    return news_list

def fetch_article_content(url):
    try:
        res = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'})
        soup = BeautifulSoup(res.text, 'html.parser')
        paragraphs = soup.select("#dic_area p")
        content = "\n".join([p.get_text(strip=True) for p in paragraphs])
        return content[:1000]
    except Exception as e:
        print(f"âŒ ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        return ""

def summarize_news(title, content):
    try:
        prompt = f"ë‹¤ìŒ ë‰´ìŠ¤ ë‚´ìš©ì„ í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•´ì¤˜:\n\nì œëª©: {title}\n\në‚´ìš©: {content}"
        response = openai.ChatCompletion.create(
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": prompt}]
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        print(f"âŒ ìš”ì•½ ì‹¤íŒ¨: {e}")
        return "ìš”ì•½ ì‹¤íŒ¨"

def analyze_sentiment(title, content):
    try:
        prompt = f"ë‹¤ìŒ ë‰´ìŠ¤ê°€ íˆ¬ìì ê´€ì ì—ì„œ ê¸ì •ì ì¸ì§€, ë¶€ì •ì ì¸ì§€, ì¤‘ë¦½ì ì¸ì§€ íŒë‹¨í•´ì¤˜:\n\nì œëª©: {title}\n\në‚´ìš©: {content}"
        response = openai.ChatCompletion.create(
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": prompt}]
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        print(f"âŒ ê°ì„± ë¶„ì„ ì‹¤íŒ¨: {e}")
        return "ë¶„ì„ ì‹¤íŒ¨"

def send_telegram(title, summary, sentiment, link, press):
    message = f"""ğŸ“° <b>{title}</b>\n\n<b>ìš”ì•½:</b> {summary}\n<b>ê°ì„±:</b> {sentiment}\n<b>ì–¸ë¡ ì‚¬:</b> {press}\n\n{link}"""
    url_api = f"https://api.telegram.org/bot{BOT_TOKEN}/sendMessage"
    data = {
        'chat_id': CHAT_ID,
        'text': message,
        'parse_mode': 'HTML',
        'disable_web_page_preview': False
    }
    try:
        requests.post(url_api, data=data)
        print(f"[í…”ë ˆê·¸ë¨ ì „ì†¡ ì™„ë£Œ] {title}")
    except Exception as e:
        print(f"âŒ í…”ë ˆê·¸ë¨ ì „ì†¡ ì‹¤íŒ¨: {e}")

# âœ… í…ŒìŠ¤íŠ¸ìš© ë‰´ìŠ¤ ë£¨í”„
def news_loop():
    old_links = load_old_links()
    while True:
        print(f"[{datetime.now().strftime('%H:%M:%S')}] ì‹¤ì‹œê°„ ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘...")
        news_items = get_live_news()
        count = 0
        for title, link, press in news_items:
            if link not in old_links:
                content = fetch_article_content(link)
                summary = summarize_news(title, content)
                sentiment = analyze_sentiment(title, content)
                send_telegram(title, summary, sentiment, link, press)
                log_to_sheet(sheet, title, summary, sentiment, link, press)

                old_links.append(link)
                count += 1
                time.sleep(1)
                if count >= MAX_SEND_PER_LOOP:
                    break
        save_old_links(old_links)
        time.sleep(60)

# âœ… ì‹¤í–‰
threading.Thread(target=news_loop, daemon=True).start()

if __name__ == '__main__':
    port = int(os.environ.get("PORT", 5000))
    app.run(host='0.0.0.0', port=port)
