# -*- coding: utf-8 -*-
import threading
import requests
from bs4 import BeautifulSoup
import time
from datetime import datetime
import gspread
from oauth2client.service_account import ServiceAccountCredentials
import json
import os
from flask import Flask
import pandas as pd

# ðŸ” ë””ë§ˆë‹ˆì½” ì •ë³´
BOT_TOKEN = 'í…”ë ˆê·¸ëž¨ë´‡í† í°'
CHAT_ID = 'ì±„íŒ…ID'
SHEET_NAME = 'ë””ë§ˆë‹ˆì½” ë‰´ìŠ¤ íŠ¸ëž˜ì»¤'

# âœ… Flask ë°±ê·¸ë¼ìš´ë“œ ì„œë²„ ìœ ì§€ (Renderìš©)
app = Flask(__name__)
@app.route('/')
def home():
    return "ë””ë§ˆë‹ˆì½” ë‰´ìŠ¤ë´‡ ìž‘ë™ ì¤‘!"

def run_flask():
    app.run(host='0.0.0.0', port=10000)
threading.Thread(target=run_flask).start()

# âœ… ì¢…ëª© ë¦¬ìŠ¤íŠ¸ (cp949 ì¸ì½”ë”©)
def get_krx_stock_list():
    url = "https://kind.krx.co.kr/corpgeneral/corpList.do?method=download"
    df = pd.read_html(url, encoding='cp949')[0]
    df['ì¢…ëª©ì½”ë“œ'] = df['ì¢…ëª©ì½”ë“œ'].apply(lambda x: f"{x:06d}")
    return dict(zip(df['íšŒì‚¬ëª…'], df['ì¢…ëª©ì½”ë“œ']))

stock_dict = get_krx_stock_list()

# âœ… ì¢…ëª©ëª… ì¶”ì¶œ
def extract_stock_from_article(title, url, stock_dict):
    text = title
    try:
        res = requests.get(url, timeout=3)
        soup = BeautifulSoup(res.text, 'html.parser')
        body = soup.get_text()
        text += body
    except:
        pass

    for name in stock_dict.keys():
        if name in text:
            return name, stock_dict[name]
    return None, None

# âœ… ë‰´ìŠ¤ ìˆ˜ì§‘ (í•„í„° ì—†ì´ ì „ë¶€)
def get_naver_stock(news_list):
    url = "https://finance.naver.com/news/news_list.naver?mode=LSS2&section_id=101&section_id2=258"
    res = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'})
    soup = BeautifulSoup(res.text, "html.parser")
    for a in soup.select("dd.articleSubject a"):
        title = a.get_text(strip=True)
        link = "https://finance.naver.com" + a['href']
        news_list.append((title, link))

def get_daum_economy(news_list):
    url = "https://news.daum.net/breakingnews/economic"
    res = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'})
    soup = BeautifulSoup(res.text, "html.parser")
    for a in soup.select("ul.list_news2 a.link_txt"):
        title = a.get_text(strip=True)
        link = a['href']
        news_list.append((title, link))

def get_yna_stock(news_list):
    url = "https://www.yna.co.kr/theme-stock"
    res = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'})
    soup = BeautifulSoup(res.text, "html.parser")
    for a in soup.select("div.list-type038 a.tit-wrap"):
        title = a.get_text(strip=True)
        link = "https:" + a['href']
        news_list.append((title, link))

def get_all_news():
    collected_news = []
    threads = [
        threading.Thread(target=get_naver_stock, args=(collected_news,)),
        threading.Thread(target=get_daum_economy, args=(collected_news,)),
        threading.Thread(target=get_yna_stock, args=(collected_news,))
    ]
    for t in threads:
        t.start()
    for t in threads:
        t.join()
    unique = list({link: title for title, link in collected_news}.items())
    return unique

# âœ… êµ¬ê¸€ì‹œíŠ¸ ì—°ê²°
def connect_sheet():
    key_json = os.environ.get('GOOGLE_KEY_JSON')
    if not key_json:
        print("âŒ GOOGLE_KEY_JSON í™˜ê²½ë³€ìˆ˜ ì—†ìŒ!")
        exit()

    key_dict = json.loads(key_json)
    scope = ['https://spreadsheets.google.com/feeds', 'https://www.googleapis.com/auth/drive']
    creds = ServiceAccountCredentials.from_json_keyfile_dict(key_dict, scope)
    client = gspread.authorize(creds)
    sheet = client.open(SHEET_NAME).sheet1
    return sheet

sheet = connect_sheet()

# âœ… ì‹œíŠ¸ ê¸°ë¡
def log_to_sheet(sheet, title, link):
    now = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    sheet.append_row([now, title, link])
    print(f"[ì‹œíŠ¸ ê¸°ë¡ë¨] {title}")

# âœ… í…”ë ˆê·¸ëž¨ ì „ì†¡
def send_telegram_news(title, link):
    message = f"""[ë””ë§ˆë‹ˆì½” ë‰´ìŠ¤]

{title}
{link}
"""
    url = f"https://api.telegram.org/bot{BOT_TOKEN}/sendMessage"
    data = {
        'chat_id': CHAT_ID,
        'text': message,
        'parse_mode': 'HTML',
        'disable_web_page_preview': False
    }
    response = requests.post(url, data=data)
    print(f"[í…”ë ˆê·¸ëž¨ ì‘ë‹µ] {response.text}")

# âœ… ë‰´ìŠ¤ ìˆ˜ì§‘ ë£¨í”„

def start_news_loop():
    old_links = []
    while True:
        print(f"[{datetime.now().strftime('%H:%M:%S')}] ë£¨í”„ ëŒê³  ìžˆìŒ...")
        news = get_all_news()
        print(f"[{datetime.now().strftime('%H:%M:%S')}] ìˆ˜ì§‘ëœ ë‰´ìŠ¤ ê°œìˆ˜: {len(news)}")
        for link, title in news:
            if link not in old_links:
                stock_name, stock_code = extract_stock_from_article(title, link, stock_dict)
                if stock_name:
                    title = f"[{stock_name}] {title}"
                send_telegram_news(title, link)
                log_to_sheet(sheet, title, link)
                old_links.append(link)
                if len(old_links) > 30:
                    old_links.pop(0)
        time.sleep(60)

threading.Thread(target=start_news_loop).start()

